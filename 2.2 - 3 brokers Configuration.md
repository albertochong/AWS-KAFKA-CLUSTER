


# 3 brokers Configuration
Cluster with 3 brokes.

### Run in terminal on each 3 instances as root 

* attach and format 2 diferents EBS disks for data logs
```bash
sudo su \

# check disks on the server
lsblk 
```
![alt text](https://achong.blob.core.windows.net/gitimages/disks.PNG)


* Check if disk is empty, if return "Data" , yes is empty
```bash
file -s /dev/xvdb \
file -s /dev/xvdc
```
![alt text](https://achong.blob.core.windows.net/gitimages/disks_empty.PNG)


* Install packages to mount as xfs
#### Note on Kafka: it's better to format volumes as xfs:
#### https://kafka.apache.org/documentation/#filesystems
```bash
apt-get install -y xfsprogs

# create a partition
fdisk /dev/xvdb
fdisk /dev/xvdc

# format as xfs
mkfs.xfs -f /dev/xvdb
mkfs.xfs -f /dev/xvdc

# create data directory
mkdir /broker
mkdir /broker/data
mkdir /broker/data1

# Give permissions to data directory
chown -R ubuntu:ubuntu /broker/data
chown -R ubuntu:ubuntu /broker/data1

# mount volume
mount -t xfs /dev/xvdb /broker/data
mount -t xfs /dev/xvdc /broker/data1

# Adding to fstab
cp /etc/fstab /etc/fstab.bak # backup
echo ‘/dev/xvdb/broker/data xfs defaults 0 0’ >> /etc/fstab
echo ‘/dev/xvdc/broker/data1 xfs defaults 0 0’ >> /etc/fstab

```

* Checking disks
```bash
df -h
```

![alt text](https://achong.blob.core.windows.net/gitimages/disks_partition.PNG)


*
```bash

```


* Edit the hosts file and add the following entries 
```bash
sudo nano /etc/hosts
172.31.22.104 zookeeper1
172.31.28.226 zookeeper2
172.31.22.176 zookeeper3

172.31.3.111  broker1
172.31.5.198  broker2
172.31.10.181 broker3
```

* On each server, edit the broker config file and add the lines:
```bash
sudo nano /opt/kafka/config/server.properties
```
Name/Value   | Description
------------ | -------------
**broker.id=1** | The id of the broker. This must be set to a unique integer for each broker.By default, this integer is set to 0, but it can be any value.
**advertised.listeners=PLAINTEXT://ec2-54-151-11-145.us-west-1.compute.amazonaws.com:9092** |                                     
**auto.create.topics.enable=false** | To help undesire behavior because if set true topic are created **When a producer starts writing messages to the topic**, or **When a consumer starts reading messages from the topic** or **When any client requests metadata for the topic**
**default.replication.factor=3** we will have 3 brokers so the default replication factor should be 2 or 3| 
**min.insync.replicas=2** | number of ISR to have in order to minimize data loss
**unclean.leader.election.enable=false** | Depends business if we put false we avoid lost data because only in-sync replicas will be choosen as new leader
**log.dirs=/home/ubuntu/broker/data,/home/ubuntu/broker/data1** | 
**num.recovery.threads.per.data.dir=8** | The number of threads per data directory to be used for log recovery at startup and flushing at shutdown.This means that if num.recovery.threads.per.data.dir is set to 8, and there are 2 paths specified in log.dirs, this is a total of 16 threads
**log.flush.interval.messages=** | 
**log.flush.interval.ms=1000** | 
**log.retention.hours=168** | The minimum age of a log file to be eligible for deletion due to age,7 days is default. after this data will delete                             **zookeeper.connect=zookeeper1:2181,,zookeeper2:2181,zookeeper3:2181/kafka** | Zookeeper connection string. You can also append an optional chroot string to the urls to specify the root directory for all kafka znodes
**zookeeper.connection.timeout.ms=6000** | Timeout in ms for connecting to zookeeper



* Testing comunication between broker and Zookeepers  in each server
```bash 
# From server 1
  cd /opt/kafka/bin
  zookeeper-shell.sh zookeeper2:2181
  zookeeper-shell.sh zookeeper3:2181
  
# From server 2
  cd /opt/kafka/bin
  zookeeper-shell.sh zookeeper1:2181
  zookeeper-shell.sh zookeeper3:2181
  
# From server 3
  cd /opt/kafka/bin
  zookeeper-shell.sh zookeeper2:2181
  zookeeper-shell.sh zookeeper1:2181

``` 

* Setup Broker as service on each instance
```bash 
  * Create a file .service.Add the following lines to the file to define the broker Service
    sudo nano /etc/systemd/system/broker.service
    ################################################################################################
    [Unit]
    Description=Broker Daemon
    Documentation=https://kafka.apache.org/
    Requires=network.target
    After=network.target

    [Service]
    Type=simple
    WorkingDirectory=/opt/kafka/bin
    User=ubuntu
    Group=ubuntu
    Environment="KAFKA_HEAP_OPTS=-Xmx4g"
    ExecStart=/opt/kafka/bin/kafka-server-start.sh /opt/kafka/config/server.properties
    ExecStop=/opt/kafka/bin/kafka-server-stop.sh /opt/kafka/config/server.properties
    LimitNOFILE=100000
    TimeoutStopSec=180
    Restart=no
    #StandardOutput=append:/path_to_log/service.log
    #StandardError=append:/path_to_log/service_error.log

    [Install]
    WantedBy=multi-user.target
    ################################################################################################
    
  * Reload
    ```bash
    sudo systemctl daemon-reload
    ```
    
  * Enable Service
    ```bash
    sudo systemctl enable broker.service
    ```  
    
  * start Service
    ```bash
    sudo systemctl start broker.service
    ```
    
  * Check Service status
    ```bash
    sudo systemctl status broker.service
    ```
    
  * Check error if any
    ```bash
    sudo journalctl -n 200 -u broker.service
    ```
    
``` 

![alt text](https://achong.blob.core.windows.net/gitimages/cluster_kafka.PNG)




